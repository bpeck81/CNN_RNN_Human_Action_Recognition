{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Action Recognition with CNN + RNN\n",
    "\n",
    "This project is designed to classify human action recognition datasets with a CNN + LSTM model.\n",
    "\n",
    "Different datasets can easily be used in by adding a simple class in the datasets.py class. \n",
    "\n",
    "While I use ResNet18 as the CNN in this model, it can easliy be exchanged by different CNN architectures.\n",
    "\n",
    "Here's an image of the general model design:\n",
    "\n",
    "![alt text](architecture1.png \"Architecture\")\n",
    "\n",
    "To get started all you have to do is \n",
    "1. Download a human action recognition dataset (I use HMDB51) http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#Downloads\n",
    "2. Create a folder called \"datasets\" in the root of the project directory.\n",
    "3. Put the HMDB dataset in the datasets directory, unzip it, and rename it to \"HMDB\". Within the HMDB folder, unzip/unrar any actions you want recognized.\n",
    "4. Run this notebook!\n",
    "\n",
    "You should have a directory that looks something like this:\n",
    "\n",
    ">CNN_RNN_Human_Action_Recognition/datasets/HMDB/situp/ <br />\n",
    "CNN_RNN_Human_Action_Recognition/datasets/HMDB/walk/ <br />\n",
    "CNN_RNN_Human_Action_Recognition/datasets/HMDB/pushup/ <br />\n",
    "CNN_RNN_Human_Action_Recognition/datasets/HMDB/run/ <br />\n",
    "CNN_RNN_Human_Action_Recognition/datasets/HMDB/throw/ <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.datasets import *\n",
    "from train_model import train_model\n",
    "from test_model import test_model\n",
    "from plot_model_stats import plot_model_stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMNet(nn.Module):\n",
    "    def __init__(self, cnn_model, output_dim, hidden_dim,num_classes=10,seq_len=20 ,batch_size=1, num_lstm_layers = 1, bidirectional = False, device = 'cpu', freeze_layers=True, dropout=0, title=\"default\"):\n",
    "        super(CNNLSTMNet, self).__init__()\n",
    "        # CNN\n",
    "        self.device = device\n",
    "        self.title = title # Model Title\n",
    "        self.cnn_model = cnn_model # Torchvision CNN Model\n",
    "        \n",
    "        # Optionally Freeze CNN Layers\n",
    "        if freeze_layers:\n",
    "            for idxc, child in enumerate(self.cnn_model.children()):\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            self.cnn_model.fc.requires_grad = True\n",
    "            \n",
    "        # RNN\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim # LSTM Hidden Dimension Size\n",
    "        self.num_lstm_layers = num_lstm_layers \n",
    "        self.bidirectional = bidirectional # Sets LSTM to Uni or Bidirectional\n",
    "        self.bidirectional_mult = 2 if self.bidirectional else 1 # Used for LSTM Weight Shape\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, self.num_lstm_layers, bidirectional=self.bidirectional, dropout=dropout)\n",
    "        self.hidden2class = nn.Linear(hidden_dim*self.bidirectional_mult, num_classes) # Fully Connected Output Layer\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device),\n",
    "                    torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.seq_len*self.batch_size,x.shape[-3],x.shape[-2],-1)\n",
    "        out = self.cnn_model(x)\n",
    "        seq = out.view(self.batch_size, self.seq_len, -1).transpose_(0,1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        # LSTM input shape = (seq_len, batch, input_size)\n",
    "        out, self.hidden = self.lstm(seq.view(len(seq), self.batch_size, -1), self.hidden)\n",
    "        #LSTM output shape = (seq_len, batch, hidden_dim * bidirectional)\n",
    "        out = self.hidden2class(out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='../datasets'\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),  # 1. Resize smallest side to 256.\n",
    "        transforms.CenterCrop(224), # 2. Crop the center 224x224 pixels.\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406],  # Normalize. This is necessary when using torchnet pretrained models.\n",
    "                          std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "trainset = HMDB(30, root_dir=root_dir, transforms=train_transform) #Use ending 20 frames from each clip\n",
    "print(\"Train size:\",len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sequence(trainset):\n",
    "    print('Labels:', trainset.labels)\n",
    "    \n",
    "    # Sample the dataset\n",
    "    rand_int = np.random.randint(0,len(trainset))\n",
    "    sample_video, label = trainset[rand_int]\n",
    "    video_label = trainset.data_file_labels[rand_int]\n",
    "    num_frame = 6 # Number of frames to display\n",
    "    \n",
    "    # Display Frames\n",
    "    frames = np.asarray(transforms.ToPILImage()(sample_video[0]))\n",
    "    print('Data Shape:', sample_video.shape)\n",
    "    for i in range(1, trainset.seq_len,int(trainset.seq_len/num_frame)):\n",
    "        frame = sample_video[i]\n",
    "        for t, m, s in zip(frame, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]):\n",
    "            t.mul_(s).add_(m)\n",
    "        frame = np.asarray(transforms.ToPILImage()(frame))\n",
    "        frames = np.concatenate((frames, frame), axis=1)\n",
    "        print(i, \"frame size:\", frame.shape, 'label:', video_label)\n",
    "    print('Frame sequence')\n",
    "    print(frames.shape)\n",
    "    print('Visualize the data where the first image is normalized, and the rest are not.')\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.grid(False);\n",
    "    plt.imshow(frames)\n",
    "\n",
    "\n",
    "display_sequence(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_features = 1024 # CNN output dimensions\n",
    "num_epochs = 10\n",
    "sequence_len = trainset.seq_len # LSTM sequence length\n",
    "batch_size=3\n",
    "hidden_dim = 128 # LSTM hidden dimension size\n",
    "lstm_dropout = .1\n",
    "lstm_depth = 1\n",
    "freeze = False # True = Freeze entire CNN, False = Don't freeze any layers\n",
    "pretrain = True # Use Imagenet Pretraining with CNN\n",
    "lstm_depth_title = 'no_freeze_layers_num_lstm_layers_'+str(lstm_depth)\n",
    "num_classes = len(trainset.labels)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "title =\\\n",
    "'-dataset-'+str(trainset.title)+\\\n",
    "'-frozen-'+str(freeze)+\\\n",
    "'-num_img_features-'+ str(num_img_features) +\\\n",
    "'-num_epochs-'+str(num_epochs)+\\\n",
    "'-sequence_len-'+str(sequence_len)+\\\n",
    "'-batch_size-'+str(batch_size)+\\\n",
    "'-lstm_dim-'+str(hidden_dim)+\\\n",
    "'-lstm_depth-'+str(lstm_depth)+\\\n",
    "'-pretrain-'+str(pretrain)+\\\n",
    "'CNN-resnet18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet18 + Unidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.resnet18(pretrained=pretrain) #Choose different CNN if desired\n",
    "num_ftrs = cnn_model.fc.in_features\n",
    "cnn_model.fc = nn.Linear(num_ftrs, num_img_features) # Change CNN output layer to desired dimension\n",
    "model = CNNLSTMNet(cnn_model, num_img_features, hidden_dim, num_classes, sequence_len, batch_size, num_lstm_layers=lstm_depth, bidirectional=False, device=device, freeze_layers=freeze, dropout=lstm_dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "model_save_path= 'saved_models/'+title+'.pth'\n",
    "# Use Below to load train history and train over a saved model\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, loss_fn, batch_size, trainset, optimizer, title, device, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
